{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/GSoC/libpysal/libpysal/weights/util.py:20: UserWarning: geopandas not available. Some functionality will be disabled.\n",
      "  warn('geopandas not available. Some functionality will be disabled.')\n"
     ]
    }
   ],
   "source": [
    "from libpysal.weights import *\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "%reload_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3515, 5510)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da = xr.open_rasterio(\"nasadem_sd.tif\")\n",
    "da.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Profiling sparse weight builders for raster data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lat2SW` is very performant since it creates regular lattice without any missing values to deal with. It straight away creates diagonals and offsets which are then shipped to dia matrix constructor.\n",
    "Memory consumpttion is high due to use of `list` instead of `np.array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4093.32 MiB, increment: 3860.39 MiB\n"
     ]
    }
   ],
   "source": [
    "# memory profiling vanilla lat2SW\n",
    "%memit lat2SW(*da[0].shape, \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.22 s ± 452 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# perf-testing vanilla lat2SW\n",
    "%timeit lat2SW(*da[0].shape, \"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is `da2WSP` which uses `lat2SW` to build sparse matrix and then through boolean indexing it removes missing rows and columns from the created matrix.\n",
    "Performance takes a hit due to boolean indexing of csr matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4092.77 MiB, increment: 3859.39 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit da2WSP(da, \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.86 s ± 173 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "da2WSP(da, \"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create matrix with only non missing values in mind, `lat2SW`/`dia_matrix` was out of question since it is higly oriented towards building regular lattice.\n",
    "\n",
    "Next options were to build either [DOK](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html#scipy.sparse.dok_matrix), [CSR/CSC](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), or [COO](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix)...\n",
    "\n",
    "- It was impossible (for me) to Numba-fy `DOK` builder due to its structure and without numba it was too slow.\n",
    "- In case of `CSR/CSC` I was not able to think of a multithreaded implementation (still exploring a way to build this)\n",
    "- From the start I was biased towards `COO` as its structure is quite simple, can be numba-fied, farely easy to incorporate multithreading and fast conversion to `CSR/CSC` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2452.08 MiB, increment: 2145.65 MiB\n"
     ]
    }
   ],
   "source": [
    "# memory profiling COO based da2WSP\n",
    "%memit da2WSPn(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Profile printout saved to text file da2WSPn_mem. \n"
     ]
    }
   ],
   "source": [
    "# memory profiling\n",
    "from libpysal.weights.raster import da2WSPn\n",
    "%mprun -T da2WSPn_mem -f da2WSPn da2WSPn(da, \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /data/GSoC/libpysal/libpysal/weights/raster.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "   348    328.8 MiB    328.8 MiB   def da2WSPn(da, criterion=\"queen\", layer=None, dims=None):\n",
      "   349    328.8 MiB      0.0 MiB       layer_id = _da_checker(da, layer, dims)[0]\n",
      "   350    328.8 MiB      0.0 MiB       shape = da.shape\n",
      "   351    328.8 MiB      0.0 MiB       if layer_id:\n",
      "   352    328.8 MiB      0.0 MiB           da = da[layer_id-1:layer_id]\n",
      "   353    328.8 MiB      0.0 MiB           shape = da[0].shape\n",
      "   354    439.5 MiB    110.8 MiB       ser = da.to_series()\n",
      "   355    439.5 MiB      0.0 MiB       mask = (ser != da.nodatavals[0]).to_numpy()\n",
      "   356    551.2 MiB    111.7 MiB       ids = np.where(mask)[0]\n",
      "   357    551.2 MiB      0.0 MiB       dtype = np.uint32 if (shape[0] * shape[1]) < 65535**2 else np.uint64\n",
      "   358    551.2 MiB      0.0 MiB       n = len(ids)\n",
      "   359   1166.5 MiB      0.0 MiB       sw = sparse.coo_matrix(\n",
      "   360   1557.6 MiB   1006.4 MiB           _swbuildern(*shape, ids, mask, criterion, dtype),\n",
      "   361   1557.6 MiB      0.0 MiB           shape=(n, n),\n",
      "   362   1557.6 MiB      0.0 MiB           dtype=np.int8\n",
      "   363                                 ).tocsr()\n",
      "   364   1208.8 MiB     42.3 MiB       ser = ser[ser != da.nodatavals[0]]\n",
      "   365   1208.8 MiB      0.0 MiB       index = ser.index\n",
      "   366   1208.8 MiB      0.0 MiB       wsp = WSP(sw, index=index)\n",
      "   367   1208.8 MiB      0.0 MiB       return wsp\n"
     ]
    }
   ],
   "source": [
    "txt = open(\"da2WSPn_mem\",\"r\")\n",
    "print(txt.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46 s ± 101 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# perf-testing COO based da2WSP\n",
    "%timeit da2WSPn(da, \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 7s ± 5.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# without numba :)\n",
    "%timeit da2WSPnn(da, \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single threaded version\n",
    "def da2WSP2(da, criterion=\"queen\", layer=None, dims=None):\n",
    "    layer_id = _da_checker(da, layer, dims)[0]\n",
    "    shape = da.shape\n",
    "    if layer_id:\n",
    "        da = da[layer_id-1:layer_id]\n",
    "        shape = da[0].shape\n",
    "    ser = da.to_series()\n",
    "    mask = (ser != da.nodatavals[0]).to_numpy()\n",
    "    ids = np.where(mask)[0]\n",
    "    dtype = np.uint32 if (shape[0] * shape[1]) < 65535**2 else np.uint64\n",
    "    n = len(ids)\n",
    "    sw = sparse.coo_matrix(\n",
    "        _swbuilder(*shape, ids, mask, criterion, dtype), \n",
    "        shape=(n, n), \n",
    "        dtype=np.int8\n",
    "    ).tocsr()\n",
    "    ser = ser[ser != da.nodatavals[0]]\n",
    "    index = ser.index\n",
    "    wsp = WSP(sw, index=index)\n",
    "    return wsp\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def _swbuilder(nrows, ncols, ids, mask, criterion, dtype):\n",
    "    n = len(ids)\n",
    "    d = 4 if criterion == \"rook\" else 8\n",
    "    # id map\n",
    "    id_map = mask * 1\n",
    "    id_map[ids] = np.arange(len(ids), dtype=dtype)\n",
    "    # preallocating rows and cols \n",
    "    rows = np.empty(d*n, dtype=dtype)\n",
    "    cols = np.empty(d*n, dtype=dtype)\n",
    "    ni = 0\n",
    "    for i in range(n):\n",
    "        id_i = ids[i]\n",
    "        if ((id_i+1) % ncols) != 0:\n",
    "            og_id = id_map[id_i]\n",
    "            id_neighbor = id_map[id_i + 1]\n",
    "            if id_neighbor:\n",
    "                rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                ni += 1\n",
    "                rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                ni += 1\n",
    "        if (id_i // ncols) < (nrows - 1):\n",
    "            og_id = id_map[id_i]\n",
    "            id_neighbor = id_map[id_i+ncols]\n",
    "            if id_neighbor:\n",
    "                rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                ni += 1\n",
    "                rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                ni += 1\n",
    "        if criterion == \"queen\":\n",
    "            if (id_i // ncols) < (nrows - 1):\n",
    "                if (id_i % ncols) != 0:\n",
    "                    og_id = id_map[id_i]\n",
    "                    id_neighbor = id_map[id_i+ncols-1]\n",
    "                    if id_neighbor:\n",
    "                        rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                        ni += 1\n",
    "                        rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                        ni += 1\n",
    "                if ((id_i+1) % ncols) != 0:\n",
    "                    og_id = id_map[id_i]\n",
    "                    id_neighbor = id_map[id_i+ncols+1]\n",
    "                    if id_neighbor:\n",
    "                        rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                        ni += 1\n",
    "                        rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                        ni += 1\n",
    "    rows = rows[:ni].copy()\n",
    "    cols = cols[:ni].copy()\n",
    "    data = np.ones(ni, dtype=np.int8)\n",
    "    return (data, (rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP parallel implementation\n",
    "@njit(fastmath=True)\n",
    "def compute_chunk(nrows, ncols, ids, id_map, criterion, dtype):\n",
    "    n = len(ids)\n",
    "    d = 4 if criterion == \"rook\" else 8\n",
    "    # id map\n",
    "    rows = np.empty(d*n, dtype=dtype)\n",
    "    cols = np.empty(d*n, dtype=dtype)\n",
    "    ni = 0\n",
    "    for i in range(n):\n",
    "        id_i = ids[i]\n",
    "        if ((id_i+1) % ncols) != 0:\n",
    "            og_id = id_map[id_i]\n",
    "            id_neighbor = id_map[id_i + 1]\n",
    "            if id_neighbor:\n",
    "                rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                ni += 1\n",
    "                rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                ni += 1\n",
    "        if (id_i // ncols) < (nrows - 1):\n",
    "            og_id = id_map[id_i]\n",
    "            id_neighbor = id_map[id_i+ncols]\n",
    "            if id_neighbor:\n",
    "                rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                ni += 1\n",
    "                rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                ni += 1\n",
    "        if criterion == \"queen\":\n",
    "            if (id_i // ncols) < (nrows - 1):\n",
    "                if (id_i % ncols) != 0:\n",
    "                    og_id = id_map[id_i]\n",
    "                    id_neighbor = id_map[id_i+ncols-1]\n",
    "                    if id_neighbor:\n",
    "                        rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                        ni += 1\n",
    "                        rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                        ni += 1\n",
    "                if ((id_i+1) % ncols) != 0:\n",
    "                    og_id = id_map[id_i]\n",
    "                    id_neighbor = id_map[id_i+ncols+1]\n",
    "                    if id_neighbor:\n",
    "                        rows[ni], cols[ni] = og_id, id_neighbor\n",
    "                        ni += 1\n",
    "                        rows[ni], cols[ni] = id_neighbor, og_id\n",
    "                        ni += 1\n",
    "    rows = rows[:ni].copy()\n",
    "    cols = cols[:ni].copy()\n",
    "    return rows, cols, ni\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def chunk_generator(n_jobs, starts, ids):\n",
    "    chunk_size = starts[1] - starts[0]\n",
    "    for i in range(n_jobs):\n",
    "        start = starts[i]\n",
    "        ids_chunk = ids[start: (start + chunk_size)]\n",
    "        yield (ids_chunk,)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def _idmap(ids, mask, dtype):\n",
    "    mask = mask * 1\n",
    "    mask[ids] = np.arange(len(ids), dtype=dtype)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _swbuilder(nrows, ncols, ids, mask, criterion, dtype):\n",
    "    from joblib import Parallel, delayed\n",
    "    n = len(ids)\n",
    "    n_jobs = os.cpu_count()\n",
    "    chunk_size = n // n_jobs + 1\n",
    "    starts = np.arange(n_jobs + 1) * chunk_size\n",
    "    id_map = _idmap(ids, mask, dtype)\n",
    "    chunk = chunk_generator(n_jobs, starts, ids)\n",
    "    worker_out = Parallel(n_jobs=-1)(\n",
    "        delayed(compute_chunk)(nrows, ncols, *pars, id_map, criterion, dtype)\n",
    "        for pars in chunk\n",
    "    )\n",
    "    rows, cols, j = zip(*worker_out)\n",
    "    rows = np.hstack(rows).flatten()\n",
    "    cols = np.hstack(cols).flatten()\n",
    "    data = np.ones(np.sum(j), dtype=np.int8)\n",
    "    return (data, (rows, cols))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
